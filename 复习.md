
<!-- TOC -->

- [深度学习训练中梯度消失的原因有哪些？有哪些解决方法？](#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8E%9F%E5%9B%A0%E6%9C%89%E5%93%AA%E4%BA%9B%E6%9C%89%E5%93%AA%E4%BA%9B%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95)
    - [原因](#%E5%8E%9F%E5%9B%A0)
    - [解决方法：](#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95)
- [怎么理解过拟合以及怎么解决过拟合问题的](#%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E8%BF%87%E6%8B%9F%E5%90%88%E4%BB%A5%E5%8F%8A%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E7%9A%84)
- [L1，L2正则化](#l1l2%E6%AD%A3%E5%88%99%E5%8C%96)
- [随机森林](#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97)
- [判别式模型和生成式模型](#%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B)
- [RANSAC](#ransac)

<!-- /TOC -->

### 1. 深度学习训练中梯度消失的原因有哪些？有哪些解决方法？
#### 原因
1. 一是使用了深层网络，目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助。而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0，也就是梯度消失。

2. 二是采用了不合适的损失函数。计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid，梯度消失就会很明显，原因如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失。
#### 解决方法：
1. pre-training+fine-tunning
此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

2. 选择relu等梯度大部分落在常数上的激活函数
relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失的问题。

3. Batch Normalization
BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失的问题，或者可以理解为BN将输出从饱和区拉到了非饱和区。

4. 残差网络的捷径（shortcut）
相比较于之前的网络结构，残差网络中有很多跨层连接结构（shortcut），这样的结构在反向传播时多了反向传播的路径，可以一定程度上解决梯度消失的问题。

5. LSTM的“门（gate）”结构
LSTM全称是长短期记忆网络（long-short term memory networks），LSTM的结构设计可以改善RNN中的梯度消失的问题。主要原因在于LSTM内部复杂的“门”(gates)，LSTM通过它内部的“门”可以在更新的时候“记住”前几次训练的”残留记忆“。

### 2. 怎么理解过拟合以及怎么解决过拟合问题的
模型在训练集上表现很好，但是在验证集上却不能保持准确，也就是模型泛化能力很差。这种情况很可能是模型过拟合
造成原因主要有以下几种：
1. 训练数据集样本单一，样本不足。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。

2. 训练数据中噪声干扰过大。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。

3. 模型过于复杂。模型太复杂，已经能够死记硬背记录下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。

解决办法：
1. 在训练和建立模型的时候，从相对简单的模型开始，不要一开始就把特征做的非常多，模型参数跳的非常复杂。

2. 增加样本，要覆盖全部的数据类型。数据经过清洗之后再进行模型训练，防止噪声数据干扰模型。

3. 正则化。在模型算法中添加惩罚函数来防止过拟合。常见的有L1，L2正则化。

4. 集成学习方法bagging(如随机森林）能有效防止过拟合。

### 3. L1，L2正则化
https://blog.csdn.net/jinping_shi/article/details/52433975

### 4. 随机森林

### 5. 判别式模型和生成式模型

1. 判别式模型(Discriminative Model)：直接对条件概率p(y|x)进行建模，常见判别模型有：线性回归、决策树、支持向量机SVM、k近邻、神经网络等；
2. 生成式模型(Generative Model)：对联合分布概率p(x,y)进行建模，常见生成式模型有：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等；
生成式模型更普适；判别式模型更直接，目标性更强
生成式模型关注数据是如何产生的，寻找的是数据分布模型；判别式模型关注的数据的差异性，寻找的是分类面
由生成式模型可以产生判别式模型，但是由判别式模式没法形成生成式模型 

### 6. RANSAC
RANSAC算法的输入是一组观测数据，一个可以解释或者适应于观测数据的参数化模型，一些可信的参数。
RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证：
1.首先我们先随机假设一小组局内点为初始值。然后用此局内点拟合一个模型，此模型适应于假设的局内点，所有的未知参数都能从假设的局内点计算得出。
2.用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为它也是局内点，将局内点扩充。
3.如果有足够多的点被归类为假设的局内点，那么估计的模型就足够合理。
4.然后，用所有假设的局内点去重新估计模型，因为此模型仅仅是在初始的假设的局内点估计的，后续有扩充后，需要更新。
5.最后，通过估计局内点与模型的错误率来评估模型。
整个这个过程为迭代一次，此过程被重复执行固定的次数，每次产生的模型有两个结局：
1、要么因为局内点太少，还不如上一次的模型，而被舍弃，
2、要么因为比现有的模型更好而被选用。

